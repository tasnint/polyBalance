Perfect â€” hereâ€™s a clean, practical checklist of whatâ€™s left so PolyBalance actually works end-to-end and your /healthz checks behave correctly, without overengineering.

âœ… What youâ€™ve already done (baseline)

You already have the hard parts âœ”ï¸

Load balancer logic + strategies

Retry logic

Kubernetes deployment + service

Metrics endpoint (Prometheus-style)

Health checker goroutine in Go

/healthz and /readyz on the LB

Backend service (nginx) running in k8s

So now weâ€™re just closing the loop.

ğŸ”§ Remaining work (in correct order)
1ï¸âƒ£ Make backend /healthz actually exist (CRITICAL)

Right now nginx does not expose /healthz, so:

your health checker marks backends unhealthy

load balancer removes them

all traffic fails

You must add /healthz to nginx.
This is the single most important fix.

Minimal options:

nginx config map that returns 200 OK on /healthz

reuse / as a temporary health endpoint (acceptable for MVP)

âœ” Outcome: backends stop being marked dead.

2ï¸âƒ£ Align health checker + probes to the same endpoint

Make sure all three use the same path:

Go health checker

Kubernetes livenessProbe

Kubernetes readinessProbe

Example (consistent everywhere):

/healthz


ğŸš« No /readyz on nginx unless you actually implement it
âœ… For MVP: /healthz is fine for both probes

3ï¸âƒ£ Confirm health checker actually updates backend state

Quick sanity check in code:

When backend is unhealthy â†’ it must not be returned by strategy

When backend recovers â†’ it re-enters rotation

If needed, ensure:

Backend.SetHealthy(false) is being called

strategy only selects Healthy == true

âœ” Outcome: dynamic removal + re-add works

4ï¸âƒ£ Rebuild + redeploy PolyBalance after Go changes

Every Go change requires:

docker build -t polybalance:local .
kubectl rollout restart deployment polybalance


If you forget this, Kubernetes keeps running stale code.

5ï¸âƒ£ Validate traffic flow manually (MVP test)

Run each of these and observe behavior:

kubectl get pods
kubectl get endpoints backend
kubectl logs deployment/polybalance


Then send traffic:

curl http://localhost:<NodePort>


âœ” You should see:

successful responses

retries only on failures

no â€œno backend availableâ€ unless you kill all backends

6ï¸âƒ£ Kill pods to prove resilience (VERY resume-worthy)

Do this intentionally:

kubectl delete pod <backend-pod-name>


Expected behavior:

health checker marks it unhealthy

load balancer routes to remaining pods

no downtime

This is exactly what interviewers love.

7ï¸âƒ£ Strategy verification (simple, effective)

Manually switch strategies using CLI flag:

kubectl edit deployment polybalance


Change:

args: ["-strategy=least_connections"]


Test again with:

for i in {1..20}; do curl http://localhost:<NodePort>; done


âœ” Confirms strategy plugability works.

8ï¸âƒ£ Optional (nice-to-have, not blocking)

Only do these if you have time:

Add /readyz distinct from /healthz on LB

Log which backend handled each request

Add request ID header

Add simple rate limiting middleware

These are bonuses, not requirements.

ğŸ§  Reality check (important)

At this point your project already demonstrates:

Layer 7 routing

Kubernetes-native service discovery

Health-aware load balancing

Fault tolerance

Retry semantics

Observability hooks

Thatâ€™s well beyond an intern-level toy project.