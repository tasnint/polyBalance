üîß Remaining work (in correct order)
1Ô∏è‚É£ Make backend /healthz actually exist (CRITICAL)

Right now nginx does not expose /healthz, so:

your health checker marks backends unhealthy

load balancer removes them

all traffic fails

You must add /healthz to nginx.
This is the single most important fix.

Minimal options:

nginx config map that returns 200 OK on /healthz

reuse / as a temporary health endpoint (acceptable for MVP)

‚úî Outcome: backends stop being marked dead.

2Ô∏è‚É£ Align health checker + probes to the same endpoint

Make sure all three use the same path:

Go health checker

Kubernetes livenessProbe

Kubernetes readinessProbe

Example (consistent everywhere):

/healthz


üö´ No /readyz on nginx unless you actually implement it
‚úÖ For MVP: /healthz is fine for both probes

3Ô∏è‚É£ Confirm health checker actually updates backend state

Quick sanity check in code:

When backend is unhealthy ‚Üí it must not be returned by strategy

When backend recovers ‚Üí it re-enters rotation

If needed, ensure:

Backend.SetHealthy(false) is being called

strategy only selects Healthy == true

‚úî Outcome: dynamic removal + re-add works

4Ô∏è‚É£ Rebuild + redeploy PolyBalance after Go changes

Every Go change requires:

docker build -t polybalance:local .
kubectl rollout restart deployment polybalance


If you forget this, Kubernetes keeps running stale code.

5Ô∏è‚É£ Validate traffic flow manually (MVP test)

Run each of these and observe behavior:

kubectl get pods
kubectl get endpoints backend
kubectl logs deployment/polybalance


Then send traffic:

curl http://localhost:<NodePort>


‚úî You should see:

successful responses

retries only on failures

no ‚Äúno backend available‚Äù unless you kill all backends

6Ô∏è‚É£ Kill pods to prove resilience (VERY resume-worthy)

Do this intentionally:

kubectl delete pod <backend-pod-name>


Expected behavior:

health checker marks it unhealthy

load balancer routes to remaining pods

no downtime

This is exactly what interviewers love.

7Ô∏è‚É£ Strategy verification (simple, effective)

Manually switch strategies using CLI flag:

kubectl edit deployment polybalance


Change:

args: ["-strategy=least_connections"]


Test again with:

for i in {1..20}; do curl http://localhost:<NodePort>; done


‚úî Confirms strategy plugability works.

8Ô∏è‚É£ Optional (nice-to-have, not blocking)

Only do these if you have time:

Add /readyz distinct from /healthz on LB

Log which backend handled each request

Add request ID header

Add simple rate limiting middleware

These are bonuses, not requirements.

üß† Reality check (important)

At this point your project already demonstrates:

Layer 7 routing

Kubernetes-native service discovery

Health-aware load balancing

Fault tolerance

Retry semantics

Observability hooks